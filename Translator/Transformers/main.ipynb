{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from string import punctuation,digits,ascii_letters\n",
    "import re\n",
    "import functions as fn\n",
    "import classes as cs\n",
    "import pickle\n",
    "import tokenizers as tk\n",
    "import transformers\n",
    "from collections.abc import Iterable\n",
    "from inltk.inltk import get_similar_sentences,setup\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../GoogleTransDataset.csv')\n",
    "dataset.rename(columns={'english_sentence':'English','hindi_sentence':'Hindi'},inplace=True)\n",
    "dataset = dataset.reset_index(drop=True).dropna()\n",
    "# dataset['Hindi'] = dataset['Hindi'].apply(lambda x: x.lower().strip())\n",
    "# dataset['English'] = dataset['English'].apply(lambda x: x.lower().strip())\n",
    "# Strip punctuation\n",
    "dataset['English'] = dataset['English'].apply(lambda x: ' '.join(re.split('\\s+',x.lower().strip())))\n",
    "dataset['Hindi'] = dataset['Hindi'].apply(lambda x: ' '.join(re.split('\\s+',x.lower().strip())))\n",
    "exclude_chars_hindi = set(punctuation+digits+ascii_letters+'[२३०८१५७९४६“”\"]')\n",
    "exclude_chars_english = set(punctuation+digits+'“”')\n",
    "dataset['Hindi'] = dataset['Hindi'].apply(lambda x: ''.join([ch for ch in x if ch not in exclude_chars_hindi]))\n",
    "dataset['English'] = dataset['English'].apply(lambda x: ''.join([ch for ch in x if ch not in exclude_chars_english]))\n",
    "dataset = dataset.reset_index(drop=True).dropna()\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "dataset['English'] = dataset['English'].apply(lambda x: ' '.join(tokenizer.tokenize(x)))\n",
    "dataset['Hindi'] = dataset['Hindi'].apply(lambda x: ' '.join(tokenizer.tokenize(x)))\n",
    "dataset['Hindi'] = dataset['Hindi'].apply(lambda x: 'START__ ' + x + ' __END')\n",
    "dataset['len_english'] = dataset['English'].apply(lambda x: len(list(re.split('\\s+',x))))\n",
    "dataset['len_hindi'] = dataset['Hindi'].apply(lambda x: len(list(re.split('\\s+',x))))\n",
    "indexes = dataset[ ( dataset['len_english']>64 ) | ( dataset['len_hindi']>64 ) ].index\n",
    "dataset.drop(indexes,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the vocabularies\n",
    "english_vocab_dict = dict()\n",
    "hindi_vocab_dict = dict()\n",
    "english_vocab_dict['[UNK]'] = 0\n",
    "hindi_vocab_dict['[UNK]'] = 0\n",
    "\n",
    "for sentence in dataset['English']:\n",
    "    for word in re.split('\\s+',sentence):\n",
    "        if(word not in english_vocab_dict.keys()):\n",
    "            english_vocab_dict[word] = 1\n",
    "        else:\n",
    "            english_vocab_dict[word]+=1\n",
    "\n",
    "for sentence in dataset['Hindi']:\n",
    "    for word in re.split('\\s+',sentence):\n",
    "        if(word not in hindi_vocab_dict.keys()):\n",
    "            hindi_vocab_dict[word] = 1\n",
    "        else:\n",
    "            hindi_vocab_dict[word]+=1\n",
    "\n",
    "\n",
    "# for word,freq in english_vocab_dict.copy().items():\n",
    "#     if(freq<=2 and word != '[UNK]'):\n",
    "#         english_vocab_dict.pop(word)\n",
    "#         english_vocab_dict['[UNK]']+=1\n",
    "\n",
    "# for word,freq in hindi_vocab_dict.copy().items():\n",
    "#     if(freq<=2 and word != '[UNK]'):\n",
    "#         hindi_vocab_dict.pop(word)\n",
    "#         hindi_vocab_dict['[UNK]']+=1\n",
    "\n",
    "english_vocab_dict = {word:freq for word,freq in sorted(english_vocab_dict.items(),key=lambda pair: pair[1],reverse=True)}\n",
    "hindi_vocab_dict = {word:freq for word,freq in sorted(hindi_vocab_dict.items(),key=lambda pair: pair[1],reverse=True)}\n",
    "english_vocab_keys = list(english_vocab_dict.keys())\n",
    "hindi_vocab_keys = list(hindi_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15866, 1369)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_wtoi = dict([(word,i+1) for i,word in enumerate(english_vocab_dict)])\n",
    "h_wtoi = dict([(word,i+1) for i,word in enumerate(hindi_vocab_dict)])\n",
    "\n",
    "e_itow = dict([(i+1,word) for i,word in enumerate(english_vocab_dict)])\n",
    "h_itow = dict([(i+1,word) for i,word in enumerate(hindi_vocab_dict)])\n",
    "\n",
    "len(e_wtoi),len(h_wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_googled = fn.make_dataset(dataset,e_wtoi,h_wtoi,64)\n",
    "with open('Datasets/fullGOOGLED_E&H','rb') as f:\n",
    "    dataset_vecced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' START__ उन्होंने अपनी एम ##् ##ब ##ुल ##ें ##स बनाने के लिए स ##िर ##्फ चार सरकारी अ ##न ##ु ##बंध जी ##ते',\n",
       " ' उन्होंने अपनी एम ##् ##ब ##ुल ##ें ##स बनाने के लिए स ##िर ##्फ चार सरकारी अ ##न ##ु ##बंध जी ##ते __END')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.mapToVocab(dataset_vecced[1][-1],h_itow),fn.mapToVocab(dataset_vecced[2][-1],h_itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = 256\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "# dataset_vecced = list(dataset_vecced)\n",
    "# for i,input in enumerate(dataset_vecced):\n",
    "#     dataset_vecced[i] = input[:int(len(input)/BATCH_SIZE)*BATCH_SIZE]\n",
    "# dataset_vecced = tuple(dataset_vecced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embeddings)  (None, 64, 256)     4061696     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 256)     1024        ['encoder_embedding[0][0]']      \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " encoder_1 (TransformerEncoder)  (None, 64, 256)     2629632     ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " encoder_2 (TransformerEncoder)  (None, 64, 256)     2629632     ['encoder_1[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_3 (TransformerEncoder)  (None, 64, 256)     2629632     ['encoder_2[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_4 (TransformerEncoder)  (None, 64, 256)     2629632     ['encoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_5 (TransformerEncoder)  (None, 64, 256)     2629632     ['encoder_4[0][0]']              \n",
      "                                                                                                  \n",
      " decoder_embedding (Embeddings)  (None, 64, 256)     350464      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_6 (TransformerEncoder)  (None, 64, 256)     2629632     ['encoder_5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 256)     1024        ['decoder_embedding[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 256)     1024        ['encoder_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_1 (TransformerDecoder)  (None, 64, 256)     4207872     ['batch_normalization_1[0][0]',  \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " decoder_2 (TransformerDecoder)  (None, 64, 256)     4207872     ['decoder_1[0][0]',              \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " decoder_3 (TransformerDecoder)  (None, 64, 256)     4207872     ['decoder_2[0][0]',              \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " decoder_4 (TransformerDecoder)  (None, 64, 256)     4207872     ['decoder_3[0][0]',              \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " decoder_5 (TransformerDecoder)  (None, 64, 256)     4207872     ['decoder_4[0][0]',              \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " decoder_6 (TransformerDecoder)  (None, 64, 256)     4207872     ['decoder_5[0][0]',              \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 256)     1024        ['decoder_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_12 (Sequential)     (None, 64, 1369)     3331417     ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48,772,697\n",
      "Trainable params: 48,770,649\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = tf.keras.layers.Input((SEQ_LEN,))\n",
    "decoder_input = tf.keras.layers.Input((SEQ_LEN,))\n",
    "x= cs.Embeddings(len(english_vocab_keys),EMBED_DIMS,SEQ_LEN,name='encoder_embedding')(encoder_input)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "y= cs.Embeddings(len(hindi_vocab_keys),EMBED_DIMS,SEQ_LEN,name='decoder_embedding')(decoder_input)\n",
    "y = tf.keras.layers.BatchNormalization()(y)\n",
    "\n",
    "for i in range(6):\n",
    "    x = cs.TransformerEncoder(6,EMBED_DIMS,2048,name=f'encoder_{i+1}')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "for i in range(6):\n",
    "    y = cs.TransformerDecoder(2048,6,EMBED_DIMS,name=f'decoder_{i+1}')(y,x)\n",
    "\n",
    "y = tf.keras.layers.BatchNormalization()(y)\n",
    "dense_output = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(units=2048,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(units=len(hindi_vocab_keys),activation='softmax')\n",
    "])(y)\n",
    "\n",
    "transformer = tf.keras.Model(inputs=[encoder_input,decoder_input],outputs=dense_output)\n",
    "transformer.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=0.8,global_clipnorm=1,learning_rate=1e-5),loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = tf.data.Dataset.from_tensor_slices((dataset_vecced[0],dataset_vecced[1]))\n",
    "Ytrain = tf.data.Dataset.from_tensor_slices(dataset_vecced[2])\n",
    "training_dataset = tf.data.Dataset.zip((Xtrain,Ytrain)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "training_dataset = training_dataset.take(2667)\n",
    "training_dataset = training_dataset.shuffle(buffer_size=len(training_dataset),reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev lr = 1e-3\n",
    "# history = transformer.fit(x=training_dataset,epochs=30,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.save_model(transformer,'JOD_MODELS/model_E&Htoken.h5')\n",
    "transformer = tf.keras.models.load_model('GOOD_MODELS/model_E&Htoken_good.h5',custom_objects={'Embeddings':cs.Embeddings,'TransformerEncoder':cs.TransformerEncoder,'TransformerDecoder':cs.TransformerDecoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आप क्या कर रहे हैं'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.translate('what are you doing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'शिक्षा सफलता की कुंजी है'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.translate('education is the key to success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'वह केवल एकमात्र है जो इस बारे में आप विषय के बारे में बता सकते हैं'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.translate('he is the only one who can tell you about this topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मैं बहुत खुश हूँ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.translate('I am very happy')fn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
